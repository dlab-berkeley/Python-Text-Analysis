{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794bac7d-1acb-43a7-8e17-1c9fa6e007e1",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Fundamentals, Part 1 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "270c7730-8f60-4449-9920-6030b6dcd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f446a0-61d7-4055-9acb-34b010f8572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Use pandas to import Tweets\n",
    "csv_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8599eb-1e4d-43db-b62c-dce158a18ce0",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 1: Preprocessing with Multiple Steps\n",
    "\n",
    "So far we've learned a few preprocessing operations, let's put them together in a function! This function would be a handy one to refer to if you happen to have some messy English text data, and you want to process it with a single function. \n",
    "\n",
    "The example text data for challenge 1 has been read in. Write a function to:\n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters\n",
    "\n",
    "Feel free to recycle the codes we've used above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bec1d6-ae54-46cb-9db8-0e56b92a30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "challenge1_path = '../data/example1.txt'\n",
    "\n",
    "with open(challenge1_path, 'r') as file:\n",
    "    challenge1 = file.read()\n",
    "    \n",
    "print(challenge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdafd4fc-a816-405f-bfc5-d08285fd4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Select characters not in puncutaion\n",
    "    no_punct = []\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct.append(char)\n",
    "\n",
    "    # Join the characters into a string\n",
    "    text_no_punct = ''.join(no_punct)   \n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8773e7-208c-4272-97e2-061c97860438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a text file that has some extra blankspace at the start and end blankspace is a catchall term for spaces tabs newlines and a bunch of other things that computers distinguish but to us all look like spaces tabs and newlines the python method called strip only catches blankspace at the start and end of a string but it wont catch it in the middle for example in this sentence once again regular expressions will help us with this'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a pattern in regex\n",
    "blankspace_pattern = r'\\s+'\n",
    "\n",
    "# Write a replacement for the pattern identfied\n",
    "blankspace_repl = ' '\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Lowercase the input text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove puncutuation \n",
    "    text = remove_punct(text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "\n",
    "    # Strip away whitespace at both ends\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "clean_text(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57a143-8876-4305-b3f1-c77001296919",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 2: Remove Stop Words\n",
    "\n",
    "We have known how `nltk` and `spaCy` work as NLP packages. We've also demostrated how to identify stop words with each package. \n",
    "\n",
    "Let's write **two** functions to remove stop words from our text data. \n",
    "\n",
    "- Complete the function for stop words removal using `nltk`\n",
    "    - The starter code requires two arguments: the raw text input and a list of predefined stop words\n",
    "- Complete the function for stop words removal using `spaCy`\n",
    "    - The starter code requires one argument: the raw text input\n",
    " \n",
    "A little reminder before we dive in: both functions take raw text as input, so that's a signal to perform tokenization on the raw text first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f743402-ace4-4b3b-8175-ba8b7ae87197",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopword_nltk(raw_text, stopword):\n",
    "    \n",
    "    # STEP 1: tokenization with nltk\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    \n",
    "    # STEP 2: filter out tokens in the stop word list\n",
    "    text = [token for token in tokens if token not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c052ec5-2f03-4c45-9f3e-ac0bbf29da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_stopword_spacy(raw_text):\n",
    "\n",
    "    # STEP 1: apply the nlp pipeline\n",
    "    doc = nlp(raw_text)\n",
    "    \n",
    "    # STEP 2: filter out tokens that are stop words\n",
    "    text = [token.text for token in doc if token.is_stop is False]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43a93c65-7fb2-40c5-9fc4-bb63fba5c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets['text'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1077f4d-e1d5-4d22-a05a-5b8370888aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " '.',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopword_nltk(text, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ec4a692-fa53-4406-a7f7-3ee5e7e9811d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica',\n",
       " 'missed',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'Men',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " '.',\n",
       " 'https://t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopword_spacy(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9a6e3-4658-4556-9bc6-40e58b2045cd",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 3: Find the Word Boundary\n",
    "\n",
    "Now that we know BERT tokenization would often return subwords. Let's try a few more examples! \n",
    "\n",
    "Does the result make sense to you? What do you think is the correct word boundary to split the following words into subwords? \n",
    "\n",
    "Also feel free to read more about limitations of the WordPiece algorithm. For instance, [this blog post](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99) dives into why would it fail, and [this one](https://tinkerd.net/blog/machine-learning/bert-tokenization/#demo-bert-tokenizer) introduces the mechanism underlying the algoritm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36437fcd-60e1-4e17-89c3-98a4c24b060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingyu/anaconda3/envs/dlab-text-analysis/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f90b0867-06bb-4641-95b7-0eb584192b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    '''Tokenzie the input string with BERT'''\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    return print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4231fc31-0683-41f3-859c-6b6a3618c03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dl', '##ab']\n",
      "['co', '##vid']\n",
      "['hug', '##ga', '##ble']\n",
      "['37', '##8']\n"
     ]
    }
   ],
   "source": [
    "# Abbreviations\n",
    "get_tokens('dlab')\n",
    "\n",
    "# OOV\n",
    "get_tokens('covid')\n",
    "\n",
    "# Prefix\n",
    "get_tokens('huggable')\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')\n",
    "\n",
    "# YOUR EXAMPLE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
